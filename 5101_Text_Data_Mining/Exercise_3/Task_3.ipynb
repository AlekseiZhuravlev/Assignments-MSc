{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3.2"
      ],
      "metadata": {
        "id": "FgvwyIJFtEtq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNcegJvI5W_7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# download the json data file\n",
        "res = requests.get('https://raw.githubusercontent.com/tobideusser/kpi-edgar/main/data/kpi_edgar.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = res.json()\n",
        "\n",
        "# print the data specification\n",
        "print(data[0].keys())\n",
        "print(data[0]['segments'][0].keys())\n",
        "print(data[0]['segments'][0]['sentences'][0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1e8_0z65xy9",
        "outputId": "ba6ad7d0-3bfb-4c30-e71a-f12a95665244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id_', 'segments'])\n",
            "dict_keys(['id_', 'value', 'sentences'])\n",
            "dict_keys(['id_', 'value', 'split_type', 'unique_id', 'words', 'entities_anno_iobes', 'entities_anno_iobes_ids', 'entities_anno', 'entities_anno_secondary', 'relations_anno', 'relations_anno_secondary'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['segments'][0]['sentences'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9wZpK3I5_ej",
        "outputId": "73c5f577-c3f8-41c7-99c5-acf5f7d0b042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id_': 4,\n",
              " 'value': '(4) Includes $ 6.7 billion of revenue recognized in 2021 that was included in deferred revenue as of September 26, 2020, $ 5.0 billion of revenue recognized in 2020 that was included in deferred revenue as of September 28, 2019, and $ 5.9 billion of revenue recognized in 2019 that was included in deferred revenue as of September 29, 2018.',\n",
              " 'split_type': 'train',\n",
              " 'unique_id': 'AAPL_10-K_0000320193-21-000105.txt_15_4',\n",
              " 'words': [{'id_': 0,\n",
              "   'value': '(',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 1,\n",
              "   'value': '4',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 4.0,\n",
              "   'value_masked': '<NUM>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': None,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 2,\n",
              "   'value': ')',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 3,\n",
              "   'value': 'Includes',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 4,\n",
              "   'value': '$',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 5,\n",
              "   'value': '6.7',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 67.0,\n",
              "   'value_masked': '<NUM_CY>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': True,\n",
              "   'unit': 'Billion USD',\n",
              "   'multiplier': 1000000000.0},\n",
              "  {'id_': 6,\n",
              "   'value': 'billion',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 7,\n",
              "   'value': 'of',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 8,\n",
              "   'value': 'revenue',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 9,\n",
              "   'value': 'recognized',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 10,\n",
              "   'value': 'in',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 11,\n",
              "   'value': '2021',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 12,\n",
              "   'value': 'that',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 13,\n",
              "   'value': 'was',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 14,\n",
              "   'value': 'included',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 15,\n",
              "   'value': 'in',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 16,\n",
              "   'value': 'deferred',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 17,\n",
              "   'value': 'revenue',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 18,\n",
              "   'value': 'as',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 19,\n",
              "   'value': 'of',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 20,\n",
              "   'value': 'September',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 21,\n",
              "   'value': '26',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 26.0,\n",
              "   'value_masked': '<NUM>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': None,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 22,\n",
              "   'value': ',',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 23,\n",
              "   'value': '2020',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 24,\n",
              "   'value': ',',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 25,\n",
              "   'value': '$',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 26,\n",
              "   'value': '5.0',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 50.0,\n",
              "   'value_masked': '<NUM_CY>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': True,\n",
              "   'unit': 'Billion USD',\n",
              "   'multiplier': 1000000000.0},\n",
              "  {'id_': 27,\n",
              "   'value': 'billion',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 28,\n",
              "   'value': 'of',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 29,\n",
              "   'value': 'revenue',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 30,\n",
              "   'value': 'recognized',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 31,\n",
              "   'value': 'in',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 32,\n",
              "   'value': '2020',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 33,\n",
              "   'value': 'that',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 34,\n",
              "   'value': 'was',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 35,\n",
              "   'value': 'included',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 36,\n",
              "   'value': 'in',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 37,\n",
              "   'value': 'deferred',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 38,\n",
              "   'value': 'revenue',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 39,\n",
              "   'value': 'as',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 40,\n",
              "   'value': 'of',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 41,\n",
              "   'value': 'September',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 42,\n",
              "   'value': '28',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 28.0,\n",
              "   'value_masked': '<NUM>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': None,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 43,\n",
              "   'value': ',',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 44,\n",
              "   'value': '2019',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 45,\n",
              "   'value': ',',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 46,\n",
              "   'value': 'and',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 47,\n",
              "   'value': '$',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 48,\n",
              "   'value': '5.9',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 59.0,\n",
              "   'value_masked': '<NUM_CY>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': True,\n",
              "   'unit': 'Billion USD',\n",
              "   'multiplier': 1000000000.0},\n",
              "  {'id_': 49,\n",
              "   'value': 'billion',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 50,\n",
              "   'value': 'of',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 51,\n",
              "   'value': 'revenue',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 52,\n",
              "   'value': 'recognized',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 53,\n",
              "   'value': 'in',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 54,\n",
              "   'value': '2019',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 55,\n",
              "   'value': 'that',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 56,\n",
              "   'value': 'was',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 57,\n",
              "   'value': 'included',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 58,\n",
              "   'value': 'in',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 59,\n",
              "   'value': 'deferred',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 60,\n",
              "   'value': 'revenue',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 61,\n",
              "   'value': 'as',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 62,\n",
              "   'value': 'of',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 63,\n",
              "   'value': 'September',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 64,\n",
              "   'value': '29',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': 29.0,\n",
              "   'value_masked': '<NUM>',\n",
              "   'is_numeric': True,\n",
              "   'is_currency': None,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 65,\n",
              "   'value': ',',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 66,\n",
              "   'value': '2018',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None},\n",
              "  {'id_': 67,\n",
              "   'value': '.',\n",
              "   'prefix': None,\n",
              "   'suffix': None,\n",
              "   'info': None,\n",
              "   'value_numeric': None,\n",
              "   'value_masked': None,\n",
              "   'is_numeric': False,\n",
              "   'is_currency': False,\n",
              "   'unit': None,\n",
              "   'multiplier': None}],\n",
              " 'entities_anno_iobes': ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'S-cy',\n",
              "  'O',\n",
              "  'O',\n",
              "  'S-kpi',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'S-py',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'S-py1',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " 'entities_anno_iobes_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  28,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  36,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  40,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'entities_anno': [{'type_': 'cy',\n",
              "   'start': 5,\n",
              "   'end': 6,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None},\n",
              "  {'type_': 'kpi',\n",
              "   'start': 8,\n",
              "   'end': 9,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None},\n",
              "  {'type_': 'py1',\n",
              "   'start': 48,\n",
              "   'end': 49,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None},\n",
              "  {'type_': 'py',\n",
              "   'start': 26,\n",
              "   'end': 27,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None}],\n",
              " 'entities_anno_secondary': [{'type_': 'cy',\n",
              "   'start': 5,\n",
              "   'end': 6,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None},\n",
              "  {'type_': 'kpi',\n",
              "   'start': 8,\n",
              "   'end': 9,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None},\n",
              "  {'type_': 'py1',\n",
              "   'start': 48,\n",
              "   'end': 49,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None},\n",
              "  {'type_': 'py',\n",
              "   'start': 26,\n",
              "   'end': 27,\n",
              "   '_value': None,\n",
              "   'score': None,\n",
              "   'embedding': None}],\n",
              " 'relations_anno': [{'type_': 'matches',\n",
              "   'head_idx': 0,\n",
              "   'tail_idx': 1,\n",
              "   'score': None,\n",
              "   'head_entity': None,\n",
              "   'tail_entity': None,\n",
              "   'is_symmetric': True,\n",
              "   'embedding': None},\n",
              "  {'type_': 'matches',\n",
              "   'head_idx': 1,\n",
              "   'tail_idx': 2,\n",
              "   'score': None,\n",
              "   'head_entity': None,\n",
              "   'tail_entity': None,\n",
              "   'is_symmetric': True,\n",
              "   'embedding': None},\n",
              "  {'type_': 'matches',\n",
              "   'head_idx': 1,\n",
              "   'tail_idx': 3,\n",
              "   'score': None,\n",
              "   'head_entity': None,\n",
              "   'tail_entity': None,\n",
              "   'is_symmetric': True,\n",
              "   'embedding': None}],\n",
              " 'relations_anno_secondary': [{'type_': 'matches',\n",
              "   'head_idx': 0,\n",
              "   'tail_idx': 1,\n",
              "   'score': None,\n",
              "   'head_entity': None,\n",
              "   'tail_entity': None,\n",
              "   'is_symmetric': True,\n",
              "   'embedding': None},\n",
              "  {'type_': 'matches',\n",
              "   'head_idx': 1,\n",
              "   'tail_idx': 2,\n",
              "   'score': None,\n",
              "   'head_entity': None,\n",
              "   'tail_entity': None,\n",
              "   'is_symmetric': True,\n",
              "   'embedding': None},\n",
              "  {'type_': 'matches',\n",
              "   'head_idx': 1,\n",
              "   'tail_idx': 3,\n",
              "   'score': None,\n",
              "   'head_entity': None,\n",
              "   'tail_entity': None,\n",
              "   'is_symmetric': True,\n",
              "   'embedding': None}]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQBnMHGA6_pb",
        "outputId": "abf4ae9b-c416-49d5-ce71-0d17e8aab7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sacremoses) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subword tokenization\n",
        "\n",
        "Let us apply two tokenizer models, BertTokenizer and moses, and compare the output."
      ],
      "metadata": {
        "id": "oh3jR7ottqsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# example of two tokenizers for Sub-Word Tokenizing\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer_moses = torchtext.data.get_tokenizer(\"moses\")\n",
        "\n",
        "sentence = data[0]['segments'][0]['sentences'][0]['value']\n",
        "print(sentence)\n",
        "print('moses\\n', tokenizer_moses(sentence))\n",
        "print('bert', '\\n', tokenizer_bert.tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqBpsx62_kFL",
        "outputId": "6997dc84-3a22-49bf-c50e-c7f1d65d4e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4) Includes $ 6.7 billion of revenue recognized in 2021 that was included in deferred revenue as of September 26, 2020, $ 5.0 billion of revenue recognized in 2020 that was included in deferred revenue as of September 28, 2019, and $ 5.9 billion of revenue recognized in 2019 that was included in deferred revenue as of September 29, 2018.\n",
            "moses\n",
            " ['(', '4', ')', 'Includes', '$', '6.7', 'billion', 'of', 'revenue', 'recognized', 'in', '2021', 'that', 'was', 'included', 'in', 'deferred', 'revenue', 'as', 'of', 'September', '26', ',', '2020', ',', '$', '5.0', 'billion', 'of', 'revenue', 'recognized', 'in', '2020', 'that', 'was', 'included', 'in', 'deferred', 'revenue', 'as', 'of', 'September', '28', ',', '2019', ',', 'and', '$', '5.9', 'billion', 'of', 'revenue', 'recognized', 'in', '2019', 'that', 'was', 'included', 'in', 'deferred', 'revenue', 'as', 'of', 'September', '29', ',', '2018', '.']\n",
            "bert \n",
            " ['(', '4', ')', 'includes', '$', '6', '.', '7', 'billion', 'of', 'revenue', 'recognized', 'in', '2021', 'that', 'was', 'included', 'in', 'def', '##erre', '##d', 'revenue', 'as', 'of', 'september', '26', ',', '2020', ',', '$', '5', '.', '0', 'billion', 'of', 'revenue', 'recognized', 'in', '2020', 'that', 'was', 'included', 'in', 'def', '##erre', '##d', 'revenue', 'as', 'of', 'september', '28', ',', '2019', ',', 'and', '$', '5', '.', '9', 'billion', 'of', 'revenue', 'recognized', 'in', '2019', 'that', 'was', 'included', 'in', 'def', '##erre', '##d', 'revenue', 'as', 'of', 'september', '29', ',', '2018', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem with tokenization\n",
        "\n",
        "The provided dataset was already tokenized using a BertTokenizer combined with a regex algorithm to properly extract numerical data. \n",
        "\n",
        "But as we can see, original BertTokenizer breaks the numbers with a decimal part: \n",
        "\n",
        "'6.7' -> '6', '.', '7'\n",
        "\n",
        "We could use moses tokenizer, but for some sentences it produces a different token sequence from the dataset, so we will have to manually relabel the dataset.\n",
        "\n",
        "To actually use the provided labels, we will have to use the pre-tokenized words from the dataset."
      ],
      "metadata": {
        "id": "3J5iGfaO01wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "We extract the words and labels from the json file and rearrange them into lists"
      ],
      "metadata": {
        "id": "cJf0s-yxtubr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the data\n",
        "\n",
        "sentence_list = []\n",
        "iobes_ids_list = []\n",
        "for data_entry in data:\n",
        "\n",
        "  for segment in data_entry['segments']:\n",
        "\n",
        "    if segment and 'sentences' in segment and segment['sentences']:\n",
        "      # print(segment)\n",
        "      for sentence in segment['sentences']:\n",
        "\n",
        "        if 'words' in sentence:\n",
        "          sentence_words = []\n",
        "          sentence_labels = sentence['entities_anno_iobes_ids']\n",
        "\n",
        "          for word in sentence['words']:\n",
        "            sentence_words.append(word['value'])\n",
        "            \n",
        "          if sentence_words and sentence_labels:\n",
        "            sentence_list.append(sentence_words)\n",
        "            iobes_ids_list.append(sentence_labels)"
      ],
      "metadata": {
        "id": "6YjLCaGQ1S37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results of preprocessing\n",
        "\n",
        "print('N of sentences:', len(sentence_list), '| N of labels:', len(iobes_ids_list))\n",
        "\n",
        "print(sentence_list[-5:])\n",
        "print(iobes_ids_list[-5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyM8wYf06x_Q",
        "outputId": "4556bd92-7912-4cf7-d8c1-76f63fee1b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N of sentences: 1158 | N of labels: 1158\n",
            "[['The', 'table', 'below', 'provides', 'additional', 'detail', 'for', 'those', '21', 'projects', ',', 'which', 'total', '$', '3,181', 'million', '.'], ['(', '2', ')', 'Includes', 'premiums', 'of', '$', '148', 'million', 'in', '2020', '.'], ['(', '3', ')', 'Includes', 'premiums', 'of', '$', '87', 'million', 'in', '2020', 'and', '$', '92', 'million', 'in', '2019', '.'], ['The', 'reconciliation', 'between', 'income', 'tax', 'expense', '(', 'credit', ')', 'and', 'a', 'theoretical', 'U.S', '.', 'tax', 'computed', 'by', 'applying', 'a', 'rate', 'of', '21', 'percent', 'for', '2020', ',', '2019', 'and', '2018', 'is', 'as', 'follows', ':', '(', '1', ')', '2020', 'includes', 'the', 'impact', 'of', 'an', 'increase', 'in', 'valuation', 'allowance', 'of', '$', '647', 'million', 'in', 'non-U.S', '.', 'and', '$', '115', 'million', 'in', 'U.S', '.', 'state', 'jurisdictions', '.'], ['(', '2', ')', '2019', 'includes', 'taxes', 'less', 'than', 'the', 'theoretical', 'U.S', '.', 'tax', 'of', '$', '773', 'million', 'from', 'Norway', 'operations', 'and', 'the', 'sale', 'of', 'upstream', 'assets', ',', '$', '657', 'million', 'from', 'a', 'tax', 'rate', 'change', 'in', 'Alberta', ',', 'Canada', ',', 'and', '$', '268', 'million', 'from', 'an', 'adjustment', 'to', 'a', 'prior', 'year', 'tax', 'position', '.']]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 29, 30, 31, 0, 0, 0, 0, 8, 0, 0], [0, 0, 0, 0, 28, 0, 0, 8, 0, 0, 0, 0], [0, 0, 0, 0, 28, 0, 0, 8, 0, 0, 0, 0, 0, 36, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 27, 0, 0, 20, 0, 0, 44, 0, 0, 0, 20, 0, 0, 41, 42, 42, 43, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 36, 0, 0, 41, 42, 42, 42, 42, 42, 42, 43, 0, 0, 36, 0, 0, 0, 41, 42, 42, 42, 43, 0, 0, 0, 0, 0, 36, 0, 0, 0, 41, 42, 42, 42, 42, 43, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model setup"
      ],
      "metadata": {
        "id": "9KiNlbsitzRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle as pkl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "_NeZqX6yAm_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    CLS = [101]\n",
        "    SEP = [102]\n",
        "    VALUE_TOKEN = [0]\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    VAL_BATCH_SIZE = 8\n",
        "    EPOCHS = 4\n",
        "    TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)"
      ],
      "metadata": {
        "id": "niGAEgEkCsci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset setup\n",
        "\n",
        "Here we prepare the dataset class for training of BERT. \n",
        "\n",
        "1) We tokenize the words using BertTokenizer\n",
        "\n",
        "2) We add special symbols, and pad the sentences and labels so that each sentence has 128 words/labels\n",
        "\n",
        "3) We mask the original words with 1s and padded words with 0s"
      ],
      "metadata": {
        "id": "_XiCh5TEuMwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "  \n",
        "  def __init__(self, texts, tags):\n",
        "    \n",
        "    #Texts: [['Diana', 'is', 'a', 'girl], ['she', 'plays', 'football']]\n",
        "    #tags: [[0, 1, 2, 5], [1, 3, 5]]\n",
        "    \n",
        "    self.texts = texts\n",
        "    self.tags = tags\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    texts = self.texts[index]\n",
        "    tags = self.tags[index]\n",
        "\n",
        "    #Tokenise\n",
        "    ids = []\n",
        "    target_tag = []\n",
        "\n",
        "    for i, s in enumerate(texts):\n",
        "        inputs = Config.TOKENIZER.encode(s, add_special_tokens=False)\n",
        "     \n",
        "        input_len = len(inputs)\n",
        "        ids.extend(inputs)\n",
        "        target_tag.extend(input_len * [tags[i]])\n",
        "    \n",
        "    #To Add Special Tokens, subtract 2 from MAX_LEN\n",
        "    ids = ids[:Config.MAX_LEN - 2]\n",
        "    target_tag = target_tag[:Config.MAX_LEN - 2]\n",
        "\n",
        "    #Add Sepcial Tokens\n",
        "    ids = Config.CLS + ids + Config.SEP\n",
        "    target_tags = Config.VALUE_TOKEN + target_tag + Config.VALUE_TOKEN\n",
        "\n",
        "    mask = [1] * len(ids)\n",
        "    token_type_ids = [0] * len(ids)\n",
        "\n",
        "    #Add Padding if the input_len is small\n",
        "\n",
        "    padding_len = Config.MAX_LEN - len(ids)\n",
        "    ids = ids + ([0] * padding_len)\n",
        "    target_tags = target_tags + ([0] * padding_len)\n",
        "    mask = mask + ([0] * padding_len)\n",
        "    token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "\n",
        "    return {\n",
        "        \"ids\" : torch.tensor(ids, dtype=torch.long),\n",
        "        \"mask\" : torch.tensor(mask, dtype=torch.long),\n",
        "        \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        \"target_tags\" : torch.tensor(target_tags, dtype=torch.long)\n",
        "      }"
      ],
      "metadata": {
        "id": "Wc9J2yUGCijs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-test split"
      ],
      "metadata": {
        "id": "XTHdihRuuECW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Train Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_list, iobes_ids_list, test_size=0.15)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)\n",
        "\n",
        "#Create DataLoaders\n",
        "train_dataset = Dataset(texts = X_train, tags = y_train)\n",
        "val_dataset = Dataset(texts = X_val, tags = y_val)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)\n",
        "\n",
        "for i, data_ in enumerate(train_data_loader):\n",
        "    print(data_)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duhVXJju7Tfi",
        "outputId": "f633c1cd-b5ad-4e1a-c9ac-7a8d83a2992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': tensor([[ 101,  100, 1022,  ...,    0,    0,    0],\n",
            "        [ 101,  100, 1999,  ...,    0,    0,    0],\n",
            "        [ 101,  100, 1997,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101,  100, 3858,  ...,    0,    0,    0],\n",
            "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
            "        [ 101,  100, 2760,  ...,    0,    0,    0]]), 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]]), 'target_tags': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [ 0, 28,  0,  ...,  0,  0,  0],\n",
            "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [ 0,  0,  0,  ...,  0,  0,  0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for training and evaluation"
      ],
      "metadata": {
        "id": "ztWDX4rut_Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_fn(train_data_loader, model, optimizer, device, scheduler):\n",
        "    #Train the Model\n",
        "    model.train()\n",
        "    loss_ = 0\n",
        "    for data in tqdm(train_data_loader, total = len(train_data_loader)):\n",
        "        for i, j in data.items():\n",
        "            data[i] = j.to(device)\n",
        "\n",
        "        #Backward Propagation\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        loss_ += loss.item()\n",
        "    return model, loss_ / len(train_data_loader)\n",
        "\n",
        "def val_fn(val_data_loader, model, optimizer, device, scheduler):\n",
        "    model.eval()\n",
        "    loss_ = 0\n",
        "    for data in tqdm(val_data_loader, total = len(val_data_loader)):\n",
        "        for i, j in data.items():\n",
        "            data[i] = j.to(device)\n",
        "        _, loss = model(**data)\n",
        "        loss_ += loss.item()\n",
        "    return loss_ / len(val_data_loader)"
      ],
      "metadata": {
        "id": "m7As4M6-CG26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT model\n",
        "\n",
        "Our model includes BertModel, a dropout layer, and a linear layer with 768 inputs and an output neuron for each tag number"
      ],
      "metadata": {
        "id": "ovFdr_mKs6sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERBertModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_tag):\n",
        "        super(NERBertModel, self).__init__()\n",
        "        self.num_tag = num_tag\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out_tag = nn.Linear(768, self.num_tag)\n",
        "        \n",
        "    def forward(self, ids, mask, token_type_ids, target_tags):\n",
        "        output, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        bert_out = self.bert_drop(output) \n",
        "        tag = self.out_tag(bert_out)\n",
        "    \n",
        "        #Calculate the loss\n",
        "        Critirion_Loss = nn.CrossEntropyLoss()\n",
        "        active_loss = mask.view(-1) == 1\n",
        "        active_logits = tag.view(-1, self.num_tag)\n",
        "        active_labels = torch.where(active_loss, target_tags.view(-1), torch.tensor(Critirion_Loss.ignore_index).type_as(target_tags))\n",
        "        loss = Critirion_Loss(active_logits, active_labels)\n",
        "        return output, tag, loss"
      ],
      "metadata": {
        "id": "skyKUR3OCNvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Model Architecture\n",
        "\n",
        "num_tag = max(np.unique([item for sublist in iobes_ids_list for item in sublist])) + 1\n",
        "print(num_tag)\n",
        "model = NERBertModel(num_tag=num_tag)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyzgcOCONh3T",
        "outputId": "ecd133ab-e729-420f-8991-e689b3efab8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NERBertModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
              "  (out_tag): Linear(in_features=768, out_features=45, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for hyperparameters"
      ],
      "metadata": {
        "id": "2tNWEJ2JvNCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyperparameters(model, ff):\n",
        "\n",
        "    # ff: full_finetuning\n",
        "    if ff:\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "    else:\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "    return optimizer_grouped_parameters"
      ],
      "metadata": {
        "id": "i6t0N3HJNhSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters (optimizer, weight decay, learning rate)\n",
        "FULL_FINETUNING = False\n",
        "optimizer_grouped_parameters = get_hyperparameters(model, FULL_FINETUNING)\n",
        "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-1)\n",
        "num_train_steps = int(len(X_train) / Config.TRAIN_BATCH_SIZE * Config.EPOCHS)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "PQZ6cJ4WNvPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training of BERT"
      ],
      "metadata": {
        "id": "efXwJ7ceyrDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(Config.EPOCHS):\n",
        "    model, train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "    val_loss = val_fn(val_data_loader, model, optimizer, device, scheduler)\n",
        "    print(f\"Epoch: {epoch + 1}, Train_loss: {train_loss}, Val_loss: {val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "dOnqmd_XNyKz",
        "outputId": "4f839c0a-d328-4066-b446-9d0333c1f45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/27 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9371c857168b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch + 1}, Train_loss: {train_loss}, Val_loss: {val_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3153bb8d5edb>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#Backward Propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1163c3f21ff7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, token_type_ids, target_tags)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[0;32m-> 1019\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 )\n\u001b[1;32m    608\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    610\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each epoch of training BERT model takes about 1 hour. The loss noticeably improved after 3 iterations, but we could not wait any longer. For NER recognition we will use the untrained version."
      ],
      "metadata": {
        "id": "czahGgDmsmAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder model from Task 1\n",
        "\n",
        "We tried to train our model from Task 1, but it performed much worse than BERT and also took a long time to train. The code is almost identical to the code for BERT (replace self.bert = BertModel.from_pretrained('bert-base-uncased') with any transformer), we will not show it here."
      ],
      "metadata": {
        "id": "0SLia19xvvAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output\n",
        "Here is an example output of an untrained BERT model"
      ],
      "metadata": {
        "id": "3rJO5PdCt42f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def prediction(test_sentence, model):\n",
        "\n",
        "    Token_inputs = Config.TOKENIZER.encode(test_sentence, add_special_tokens=False)\n",
        "    test_dataset =  Dataset(test_sentence, tags= [[1] * len(test_sentence)])\n",
        "    # num_tag = len(le.classes_)\n",
        "\n",
        "    # print(test_sentence)\n",
        "    # print(Token_inputs)\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        data = test_dataset[0]\n",
        "        for i, j in data.items():\n",
        "            data[i] = j.to(device).unsqueeze(0)\n",
        "        output, tag, _ = model(**data)\n",
        "        return output\n",
        "        # print(tag)\n",
        "        # print(tag.argmax(2).cpu().numpy().reshape(-1)[1:len(Token_inputs)+1])"
      ],
      "metadata": {
        "id": "GBkMdKKEBQsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_output = prediction(X_train[0], model)\n",
        "print('BERT output:', bert_output)\n",
        "print('labels:', y_train[0])\n",
        "' '.join(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "3QzSqzEhvaIv",
        "outputId": "70d6db61-8e9f-49fc-bad1-aecda7dd2c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT output: tensor([[[-0.1689,  0.2608, -0.1726,  ...,  0.0096,  0.4342,  0.5926],\n",
            "         [ 0.0257,  0.5536,  0.4179,  ..., -0.1947,  0.3685,  0.2144],\n",
            "         [ 0.1301,  0.3480,  0.4897,  ..., -0.2039,  0.5030, -0.0019],\n",
            "         ...,\n",
            "         [-0.1569,  0.2610,  0.4485,  ..., -0.1336,  0.1430,  0.0371],\n",
            "         [-0.0396,  0.1616,  0.4300,  ..., -0.2126,  0.1323,  0.5248],\n",
            "         [-0.0524,  0.2202,  0.4112,  ..., -0.1602,  0.1982,  0.3317]]])\n",
            "labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 26, 26, 26, 26, 26, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 0, 0, 41, 42, 42, 43, 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Note 8 – Property , Plant and Equipment , Net Our property , plant and equipment , net , consisted of the following ( in millions ) : As of December 31 , 2018 , the table above included $ 1.69 billion of gross build-to-suit lease assets .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "We will make a new dataset with the results of BERT. \n",
        "\n",
        "1) For NER task, we will focus on binary classification of words in a sentence: KPI / NOT KPI. \n",
        "\n",
        "2) We will use the outputs of BERT as features for classification.\n",
        "\n",
        "3) We again pad each sentence with empty strings to set the number of words to 128. For each word we will predict whether it is KPI or not (1 or 0).\n",
        "\n",
        "3) We apply PCA to reduce the number of BERT features for a sentence from 768 * 128 = 98304 down to ~700. \n",
        "\n",
        "\n",
        "5) For classification we will use a small NN with a few linear layers."
      ],
      "metadata": {
        "id": "pZOtsZKvGC1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KPI dataset"
      ],
      "metadata": {
        "id": "6XeT2oTpxqhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class DatasetKPI:\n",
        "  \n",
        "  def __init__(self, text_inputs, tags, bert_model, n_features = None):\n",
        "    self.text_inputs = text_inputs\n",
        "    self.bert_results = []\n",
        "    self.n_features = n_features\n",
        "    \n",
        "    for text_input in tqdm(text_inputs):\n",
        "      self.bert_results.append(prediction(text_input, bert_model))\n",
        "\n",
        "    self.reduce_bert_dimensionality(self.bert_results)\n",
        "    self.tags = tags\n",
        "\n",
        "  def reduce_bert_dimensionality(self, bert_results):\n",
        "    if not self.n_features:\n",
        "      self.n_features = 0.97\n",
        "\n",
        "    pca = PCA(n_components = self.n_features)\n",
        "    bert_res = []\n",
        "    for i in bert_results:\n",
        "      # print(i)\n",
        "      bert_res.append(np.array(torch.flatten(i)))\n",
        "\n",
        "    bert_res = np.array(bert_res)\n",
        "    # print(bert_res.shape)\n",
        "    \n",
        "    self.bert_results = pca.fit_transform(np.array(bert_res))\n",
        "    self.n_features = pca.n_components_\n",
        "    print('n features:', self.n_features)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.text_inputs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    texts = self.text_inputs[index]\n",
        "    bert_results = self.bert_results[index]\n",
        "    tags = self.tags[index]\n",
        "\n",
        "    texts = texts[:Config.MAX_LEN]\n",
        "    tags = tags[:Config.MAX_LEN]\n",
        "\n",
        "    #Add Padding if the length of texts is small\n",
        "    padding_len = Config.MAX_LEN - len(texts)\n",
        "    texts = texts + ([''] * padding_len)\n",
        "\n",
        "    tags = [bool(tag) for tag in tags]\n",
        "    tags = tags + ([0] * padding_len)\n",
        "\n",
        "    return {\n",
        "        \"text\" : texts,\n",
        "        \"bert_result\" : bert_results,\n",
        "        \"tags\" : torch.tensor(tags)\n",
        "        }"
      ],
      "metadata": {
        "id": "DC5t9xATGK5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_kpi = DatasetKPI(X_train[:750], y_train[:750], model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu4yiPdEH9PR",
        "outputId": "ae96b452-986a-4408-a259-974ab31f44df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [06:24<00:00,  1.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n features: 639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/test split"
      ],
      "metadata": {
        "id": "pvCE-foisT46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_kpi = torch.utils.data.Subset(dataset_kpi, range(600))\n",
        "test_set_kpi = torch.utils.data.Subset(dataset_kpi, range(600, len(dataset_kpi)))\n",
        "\n",
        "data_loader_kpi_train = DataLoader(train_set_kpi, batch_size=Config.TRAIN_BATCH_SIZE)\n",
        "data_loader_kpi_test = DataLoader(test_set_kpi, batch_size=1)"
      ],
      "metadata": {
        "id": "rCsw9s4pMIah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN architecture\n",
        "\n",
        "For classification we use a small network with 3 linear layers and a dropout layer"
      ],
      "metadata": {
        "id": "dOZya0nOyW7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    self.linear1 = nn.Linear(dataset_kpi.n_features, 128)\n",
        "    self.linear2 = nn.Linear(128, 128)\n",
        "    self.dropout = nn.Dropout(0.7)\n",
        "    self.linear3 = nn.Linear(128, Config.MAX_LEN)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.linear1(x))\n",
        "    x = torch.relu(self.linear2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.relu(self.linear3(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "ByQ3lwQhO75t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "MnV1gJNeraWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = Network()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(clf.parameters(), lr=0.1)\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(data_loader_kpi_train, 0):\n",
        "    inputs = data['bert_result']\n",
        "    labels = data['tags']\n",
        "    # set optimizer to zero grad to remove previous epoch gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # forward propagation\n",
        "    outputs = clf(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels.float())\n",
        "    \n",
        "    # backward propagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # optimize\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    \n",
        "  # display statistics\n",
        "  print(f'{epoch + 1} loss: {running_loss}')"
      ],
      "metadata": {
        "id": "_4xNqCHKfd_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf7f07c-b076-44c4-fcc4-b140652fa488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loss: 15.076046407222748\n",
            "2 loss: 14.306605219841003\n",
            "3 loss: 13.992307186126709\n",
            "4 loss: 13.808817386627197\n",
            "5 loss: 13.687402486801147\n",
            "6 loss: 13.601940214633942\n",
            "7 loss: 13.534206986427307\n",
            "8 loss: 13.47859799861908\n",
            "9 loss: 13.43634682893753\n",
            "10 loss: 13.398392677307129\n",
            "11 loss: 13.36576509475708\n",
            "12 loss: 13.340641677379608\n",
            "13 loss: 13.323532342910767\n",
            "14 loss: 13.301436066627502\n",
            "15 loss: 13.287679135799408\n",
            "16 loss: 13.271123111248016\n",
            "17 loss: 13.256970942020416\n",
            "18 loss: 13.248479187488556\n",
            "19 loss: 13.238181471824646\n",
            "20 loss: 13.226947546005249\n",
            "21 loss: 13.221908152103424\n",
            "22 loss: 13.21670913696289\n",
            "23 loss: 13.210373282432556\n",
            "24 loss: 13.20313811302185\n",
            "25 loss: 13.19961005449295\n",
            "26 loss: 13.196849048137665\n",
            "27 loss: 13.193075716495514\n",
            "28 loss: 13.18871396780014\n",
            "29 loss: 13.187800407409668\n",
            "30 loss: 13.186232209205627\n",
            "31 loss: 13.184845447540283\n",
            "32 loss: 13.182960569858551\n",
            "33 loss: 13.180581033229828\n",
            "34 loss: 13.180615901947021\n",
            "35 loss: 13.17891538143158\n",
            "36 loss: 13.177353143692017\n",
            "37 loss: 13.177553713321686\n",
            "38 loss: 13.176912903785706\n",
            "39 loss: 13.175844550132751\n",
            "40 loss: 13.175662100315094\n",
            "41 loss: 13.173788487911224\n",
            "42 loss: 13.174540638923645\n",
            "43 loss: 13.173316061496735\n",
            "44 loss: 13.173529863357544\n",
            "45 loss: 13.172772407531738\n",
            "46 loss: 13.173166871070862\n",
            "47 loss: 13.172719776630402\n",
            "48 loss: 13.172717332839966\n",
            "49 loss: 13.172351658344269\n",
            "50 loss: 13.172002971172333\n",
            "51 loss: 13.17161911725998\n",
            "52 loss: 13.171899199485779\n",
            "53 loss: 13.171762526035309\n",
            "54 loss: 13.171355724334717\n",
            "55 loss: 13.171030402183533\n",
            "56 loss: 13.17128598690033\n",
            "57 loss: 13.170917391777039\n",
            "58 loss: 13.170904219150543\n",
            "59 loss: 13.170752167701721\n",
            "60 loss: 13.170936107635498\n",
            "61 loss: 13.170845091342926\n",
            "62 loss: 13.17061161994934\n",
            "63 loss: 13.170595705509186\n",
            "64 loss: 13.170788884162903\n",
            "65 loss: 13.170575380325317\n",
            "66 loss: 13.170562148094177\n",
            "67 loss: 13.170365989208221\n",
            "68 loss: 13.170393586158752\n",
            "69 loss: 13.170220851898193\n",
            "70 loss: 13.170327961444855\n",
            "71 loss: 13.170262694358826\n",
            "72 loss: 13.17011022567749\n",
            "73 loss: 13.170340240001678\n",
            "74 loss: 13.170373558998108\n",
            "75 loss: 13.170025050640106\n",
            "76 loss: 13.170085310935974\n",
            "77 loss: 13.170087158679962\n",
            "78 loss: 13.170000731945038\n",
            "79 loss: 13.170163571834564\n",
            "80 loss: 13.170099973678589\n",
            "81 loss: 13.170301616191864\n",
            "82 loss: 13.169947922229767\n",
            "83 loss: 13.170005679130554\n",
            "84 loss: 13.169919073581696\n",
            "85 loss: 13.17001074552536\n",
            "86 loss: 13.170046091079712\n",
            "87 loss: 13.169979572296143\n",
            "88 loss: 13.170040667057037\n",
            "89 loss: 13.169939994812012\n",
            "90 loss: 13.169821381568909\n",
            "91 loss: 13.169921040534973\n",
            "92 loss: 13.169801473617554\n",
            "93 loss: 13.169862806797028\n",
            "94 loss: 13.169804215431213\n",
            "95 loss: 13.16991239786148\n",
            "96 loss: 13.169851064682007\n",
            "97 loss: 13.169897019863129\n",
            "98 loss: 13.169884502887726\n",
            "99 loss: 13.169837713241577\n",
            "100 loss: 13.169870495796204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sample output\n",
        "\n",
        "If our prediction for a word is > 0, we round it up to 1. As a result, our output can be either 0 or 1."
      ],
      "metadata": {
        "id": "mrUds60BrUxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  inputs = test_set_kpi[i]['bert_result']\n",
        "  output = clf(torch.tensor(inputs))\n",
        "  print('prediction', (output > 0).int())\n",
        "  print('label', test_set_kpi[i]['tags'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRktJ4vS00JW",
        "outputId": "c8bef877-dbd8-4491-958f-6ae8e5e74cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
            "label tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "prediction tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 1], dtype=torch.int32)\n",
            "label tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "prediction tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
            "label tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation\n",
        "\n",
        "Let us see how many KPI indicators were classified correctly"
      ],
      "metadata": {
        "id": "nvAtFnmMrN8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hits_kpi = 0\n",
        "hits_all = 0\n",
        "misses_kpi = 0\n",
        "misses_all = 0\n",
        "\n",
        "for batch in data_loader_kpi_test:\n",
        "  for i in range(len(batch)):\n",
        "    entry = batch\n",
        "    inputs = entry['bert_result'][0]\n",
        "    label_list = entry['tags'][0].int()\n",
        "    output = clf(inputs)\n",
        "    prediction_list = (output > 0).int()\n",
        "\n",
        "    for prediction, label in zip(prediction_list, label_list):\n",
        "      if label == prediction:\n",
        "        hits_all += 1\n",
        "      if label and label == prediction:\n",
        "        hits_kpi += 1\n",
        "      if label != prediction:\n",
        "        misses_all += 1\n",
        "      if label and label != prediction:\n",
        "        misses_kpi += 1\n",
        "\n",
        "print(f'Accuracy {hits_all / (hits_all + misses_all):.3f}')\n",
        "print(f'F1 score: {hits_kpi / (hits_kpi + misses_kpi):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s72frXN4e8wP",
        "outputId": "9680f625-cb11-4f29-9a04-b041b29e7571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.841\n",
            "F1 score: 0.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mvt45I5mmBKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}