{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a325897",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing: Assignment 1\n",
    "\n",
    "In this assignment we'll practice tokenization, lemmatization and stemming\n",
    "\n",
    "- Please comment your code\n",
    "- Submissions are due Sunday at 23:59 **only** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 23.04.2023, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately \"Assignment_1_\\<Your_Name\\>.ipynb\".\n",
    "- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8bf33",
   "metadata": {},
   "source": [
    "### Task 1.1 (3 points)\n",
    "\n",
    "Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n",
    "1. num_words: The number of words in string\n",
    "2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n",
    "\n",
    "**Hint:** The string can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f14f3124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(21,\n 17,\n ['this',\n  'is',\n  'a',\n  'string',\n  'it',\n  'contains',\n  'some',\n  'words',\n  'and',\n  'some',\n  'tokens',\n  'tokens',\n  'are',\n  'the',\n  'same',\n  'as',\n  'words',\n  'but',\n  'they',\n  'are',\n  'not'],\n {'a',\n  'and',\n  'are',\n  'as',\n  'but',\n  'contains',\n  'is',\n  'it',\n  'not',\n  'same',\n  'some',\n  'string',\n  'the',\n  'they',\n  'this',\n  'tokens',\n  'words'})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_words_tokens(any_string):\n",
    "    # remove special characters from the string, and convert it to lowercase\n",
    "    any_string = ''.join(e.lower() for e in any_string if e.isalnum() or e == \" \")\n",
    "\n",
    "    # split the string into words and create a set of tokens\n",
    "    words = any_string.split()\n",
    "    tokens = set(words)\n",
    "\n",
    "    return len(words), len(tokens), words, tokens\n",
    "\n",
    "extract_words_tokens(\n",
    "    'This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b05add",
   "metadata": {},
   "source": [
    "### Task 1.2 (4 points)\n",
    "\n",
    "Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n",
    "\n",
    "**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12f48ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'some': 'some',\n 'not': 'not',\n 'is': 'be',\n 'they': 'they',\n 'as': 'a',\n 'are': 'be',\n 'string': 'string',\n 'a': 'a',\n 'words': 'word',\n 'but': 'but',\n 'contains': 'contain',\n 'it': 'it',\n 'and': 'and',\n 'this': 'this',\n 'the': 'the',\n 'same': 'same',\n 'tokens': 'token'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(any_string, file_name):\n",
    "    # load the lemmatization dictionary\n",
    "    with open(file_name, 'r') as f:\n",
    "        lemmatization_dict = {line.split('\\t')[1].strip(): line.split('\\t')[0] for line in f}\n",
    "\n",
    "    # extract tokens from the string\n",
    "    _, _, _, tokens = extract_words_tokens(any_string)\n",
    "\n",
    "    # create a dictionary with the lemmatized words\n",
    "    dictionary_of_lemmatized_words = {}\n",
    "\n",
    "    # for each token in the string, check if it's in the lemmatization dictionary\n",
    "    # if it is, add it to the dictionary with the lemma as the value\n",
    "    # if it's not, add it to the dictionary with the token as the value\n",
    "    for token in tokens:\n",
    "        if token in lemmatization_dict:\n",
    "            dictionary_of_lemmatized_words[token] = lemmatization_dict[token]\n",
    "        else:\n",
    "            dictionary_of_lemmatized_words[token] = token\n",
    "\n",
    "    return dictionary_of_lemmatized_words\n",
    "\n",
    "lemmatize('This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.', 'lemmatization-en.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266bdc4",
   "metadata": {},
   "source": [
    "### Task 1.3 (3 points)\n",
    "\n",
    "Write a function `stemmer(string)` that takes a string as input and returns a string processed with its stem.\n",
    "\n",
    "Create rules for the following cases as an example:\n",
    "\n",
    "- study - studi\n",
    "- studies - studi\n",
    "- studying - studi\n",
    "- studied - studi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b5c587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'studi', 'studi']\n"
     ]
    }
   ],
   "source": [
    "def stemmer(any_string):\n",
    "    # extract tokens from the string\n",
    "    _, _, _, tokens = extract_words_tokens(any_string)\n",
    "\n",
    "    # create a list of stemmed words\n",
    "    stemmed_words = []\n",
    "    for token in tokens:\n",
    "       # if the token ends with 'y', remove the 'y' and add 'i' to the end\n",
    "        if token.endswith('y'):\n",
    "            stemmed_words.append(token[:-1] + 'i')\n",
    "        # if the token ends with 'es', remove the 'es'\n",
    "        elif token.endswith('es'):\n",
    "            stemmed_words.append(token[:-2])\n",
    "        # if the token ends with 'ying', remove the 'ying' and add 'i' to the end\n",
    "        elif token.endswith('ying'):\n",
    "            stemmed_words.append(token[:-4] + 'i')\n",
    "        # if the token ends with 'ed', remove the 'ed'\n",
    "        elif token.endswith('ed'):\n",
    "            stemmed_words.append(token[:-2])\n",
    "        else:\n",
    "            stemmed_words.append(token)\n",
    "    return(print(stemmed_words))\n",
    "\n",
    "stemmer('Studying, studies, studied, study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
