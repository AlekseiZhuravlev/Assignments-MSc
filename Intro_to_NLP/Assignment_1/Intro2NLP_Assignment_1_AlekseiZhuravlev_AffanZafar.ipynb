{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a325897",
   "metadata": {
    "id": "5a325897"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 1\n",
    "\n",
    "In this assignment we'll practice tokenization, lemmatization and stemming\n",
    "\n",
    "- Please comment your code\n",
    "- Submissions are due Thursday at 23:59 and should be submitted **ONLY** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 14.11.2023, at 23:59)**\n",
    "- Name the file aproppriately \"Assignment_1_\\<Your_Name\\>.ipynb\".\n",
    "- Please submit **ONLY** the Jupyter Notebook file.\n",
    "- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8bf33",
   "metadata": {
    "id": "0cd8bf33"
   },
   "source": [
    "### Task 1.1 (3 points)\n",
    "\n",
    "Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n",
    "1. num_words: The number of words in string\n",
    "2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n",
    "\n",
    "**Hint:** The string can be a single word or a sentence and\n",
    " can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14f3124",
   "metadata": {
    "id": "f14f3124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a string it contains some words and some tokens tokens are the same as words but they are not : num_words: 21 and num_tokens: 17 respectively words: ['this', 'is', 'a', 'string', 'it', 'contains', 'some', 'words', 'and', 'some', 'tokens', 'tokens', 'are', 'the', 'same', 'as', 'words', 'but', 'they', 'are', 'not'] tokens: {'this', 'and', 'some', 'but', 'words', 'string', 'it', 'is', 'as', 'contains', 'not', 'they', 'a', 'the', 'are', 'tokens', 'same'}\n"
     ]
    }
   ],
   "source": [
    "def extract_words_tokens(any_string):\n",
    "    # remove special characters from the string, and convert it to lowercase\n",
    "    any_string = ''.join(e.lower() for e in any_string if e.isalnum() or e == \" \")\n",
    "\n",
    "    # split the string into words and create a set of tokens\n",
    "    words = any_string.split()\n",
    "    tokens = set(words)\n",
    "\n",
    "    print(any_string, \":\", \"num_words:\", len(words), \"and\", \"num_tokens:\", len(tokens), \"respectively\", \"words:\", words, \"tokens:\", tokens)\n",
    "\n",
    "    return len(words), len(tokens), words, tokens\n",
    "\n",
    "_ = extract_words_tokens(\n",
    "    'This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b05add",
   "metadata": {
    "id": "a4b05add"
   },
   "source": [
    "### Task 1.2 (4 points)\n",
    "\n",
    "Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n",
    "\n",
    "**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12f48ff",
   "metadata": {
    "id": "a12f48ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a string it contains some words and some tokens tokens are the same as words but they are not : num_words: 21 and num_tokens: 17 respectively words: ['this', 'is', 'a', 'string', 'it', 'contains', 'some', 'words', 'and', 'some', 'tokens', 'tokens', 'are', 'the', 'same', 'as', 'words', 'but', 'they', 'are', 'not'] tokens: {'this', 'and', 'some', 'but', 'words', 'string', 'it', 'is', 'as', 'contains', 'not', 'they', 'a', 'the', 'are', 'tokens', 'same'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'this': 'this',\n 'and': 'and',\n 'some': 'some',\n 'but': 'but',\n 'words': 'word',\n 'string': 'string',\n 'it': 'it',\n 'is': 'be',\n 'as': 'a',\n 'contains': 'contain',\n 'not': 'not',\n 'they': 'they',\n 'a': 'a',\n 'the': 'the',\n 'are': 'be',\n 'tokens': 'token',\n 'same': 'same'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(any_string, file_name):\n",
    "    # load the lemmatization dictionary\n",
    "    with open(file_name, 'r') as f:\n",
    "        lemmatization_dict = {line.split('\\t')[1].strip(): line.split('\\t')[0] for line in f}\n",
    "\n",
    "    # extract tokens from the string\n",
    "    _, _, _, tokens = extract_words_tokens(any_string)\n",
    "\n",
    "    # create a dictionary with the lemmatized words\n",
    "    dictionary_of_lemmatized_words = {}\n",
    "\n",
    "    # for each token in the string, check if it's in the lemmatization dictionary\n",
    "    # if it is, add it to the dictionary with the lemma as the value\n",
    "    # if it's not, add it to the dictionary with the token as the value\n",
    "    for token in tokens:\n",
    "        if token in lemmatization_dict:\n",
    "            dictionary_of_lemmatized_words[token] = lemmatization_dict[token]\n",
    "        else:\n",
    "            dictionary_of_lemmatized_words[token] = token\n",
    "\n",
    "    return dictionary_of_lemmatized_words\n",
    "\n",
    "lemmatize('This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.', 'lemmatization-en.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266bdc4",
   "metadata": {
    "id": "f266bdc4"
   },
   "source": [
    "### Task 1.3 (3 points)\n",
    "\n",
    "Write a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\n",
    "\n",
    "Create rules for the following forms of the verbs, Here is one example:\n",
    "\n",
    "- (Infinitive form) >> study - studi\n",
    "- (Present simple tense: Third person) >> studies - studi\n",
    "- (Continuous tense) >> studying - studi\n",
    "- (Past simple tense) >> studied - studi\n",
    "\n",
    "**Hint:** The string can be a single word or a sentence and\n",
    " can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b5c587b",
   "metadata": {
    "id": "0b5c587b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studying studies studied study : num_words: 4 and num_tokens: 4 respectively words: ['studying', 'studies', 'studied', 'study'] tokens: {'studying', 'study', 'studies', 'studied'}\n",
      "['studi', 'studi', 'studi', 'studi']\n"
     ]
    }
   ],
   "source": [
    "def stemmer(any_string):\n",
    "    # extract tokens from the string\n",
    "    _, _, _, tokens = extract_words_tokens(any_string)\n",
    "\n",
    "    # create a list of stemmed words\n",
    "    stemmed_words = []\n",
    "    for token in tokens:\n",
    "       # if the token ends with 'y', remove the 'y' and add 'i' to the end\n",
    "        if token.endswith('y'):\n",
    "            stemmed_words.append(token[:-1] + 'i')\n",
    "        # if the token ends with 'es', remove the 'es'\n",
    "        elif token.endswith('es'):\n",
    "            stemmed_words.append(token[:-2])\n",
    "        # if the token ends with 'ying', remove the 'ying' and add 'i' to the end\n",
    "        elif token.endswith('ying'):\n",
    "            stemmed_words.append(token[:-4] + 'i')\n",
    "        # if the token ends with 'ed', remove the 'ed'\n",
    "        elif token.endswith('ed'):\n",
    "            stemmed_words.append(token[:-2])\n",
    "        else:\n",
    "            stemmed_words.append(token)\n",
    "    return(print(stemmed_words))\n",
    "\n",
    "stemmer('Studying, studies, studied, study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
