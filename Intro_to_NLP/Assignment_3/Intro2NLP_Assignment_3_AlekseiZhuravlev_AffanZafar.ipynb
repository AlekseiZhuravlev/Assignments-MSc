{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981ce393",
   "metadata": {
    "id": "981ce393"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 3\n",
    "\n",
    "In this exercise we'll practice features extraction using SpaCy as well as multiclass text classification using the word embedding technique.\n",
    "\n",
    "- You can use built-in Python packages, spaCy, scikit-learn, Numpy and Pandas.\n",
    "- Please comment your code\n",
    "- Submissions are due Tuesdays at 23:59 **only** on eCampus: **Assignmnets >> Student Submissions >> Assignment 3 (Deadline: 05.12.2023, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately: \"Assignment_3_\\<Your_Name\\>.ipynb\" and submit only the Jupyter Notebook file.\n",
    "- Please use relative path, your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = bbc-news.csv, **DON'T use:** /Users/ComputerName/Username/Documents/.../bbc-news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1325c5b",
   "metadata": {
    "id": "d1325c5b"
   },
   "source": [
    "### Task 1 (2 points)\n",
    "\n",
    "Write a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "text = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\n",
    "\n",
    "return = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2mâœ” Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download 'en_core_web_sm'\n",
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92133a54",
   "metadata": {
    "id": "92133a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honk Kong', 'New York']\n"
     ]
    }
   ],
   "source": [
    "def extract_proper_nouns(my_file_name):\n",
    "    several_token_propn = []\n",
    "    # here comes your code\n",
    "\n",
    "    with open(my_file_name, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"PROPN\" and token.nbor().pos_ == \"PROPN\":\n",
    "            several_token_propn.append(token.text + \" \" + token.nbor().text)\n",
    "\n",
    "    return(several_token_propn)\n",
    "\n",
    "print(extract_proper_nouns(\"example_1.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a669e",
   "metadata": {
    "id": "8d0a669e"
   },
   "source": [
    "### Task 2 (4 points)\n",
    "\n",
    "Write a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1.\n",
    "text = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\n",
    "\n",
    "return = `{\"query\": [\"query\", \"querying\"]}`\n",
    "\n",
    "2.\n",
    "text = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\n",
    "\n",
    "return = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fab5733",
   "metadata": {
    "id": "8fab5733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['query', 'querying']}\n",
      "{'show': ['show', 'showing', 'showed']}\n"
     ]
    }
   ],
   "source": [
    "def common_lemma(my_file_name):\n",
    "    tokens_with_common_lemma = {}\n",
    "    # here comes your code\n",
    "\n",
    "    with open(my_file_name, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in tokens_with_common_lemma.keys():\n",
    "            tokens_with_common_lemma[token.lemma_] = [token.text]\n",
    "        else:\n",
    "            if token.text not in tokens_with_common_lemma[token.lemma_]:\n",
    "                tokens_with_common_lemma[token.lemma_].append(token.text)\n",
    "\n",
    "    # remove lemmas with only one token\n",
    "    for key in list(tokens_with_common_lemma.keys()):\n",
    "        if len(tokens_with_common_lemma[key]) == 1:\n",
    "            del tokens_with_common_lemma[key]\n",
    "\n",
    "    return(tokens_with_common_lemma)\n",
    "\n",
    "print(common_lemma(\"example_2_1.txt\"))\n",
    "print(common_lemma(\"example_2_2.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d27e0-56fc-40b9-b510-b2d94c90bcf3",
   "metadata": {
    "id": "d46d27e0-56fc-40b9-b510-b2d94c90bcf3"
   },
   "source": [
    "### Task 3 (1 point)\n",
    "\n",
    "a) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac56bf9-aba8-48c7-9dcd-a46d53426f82",
   "metadata": {
    "id": "fac56bf9-aba8-48c7-9dcd-a46d53426f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5057493\n"
     ]
    }
   ],
   "source": [
    "# Here comes your code\n",
    "\n",
    "with open('bbc-news.csv', 'r') as f:\n",
    "    bbc_news = f.read()\n",
    "\n",
    "print(len(bbc_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6bf1c-a802-4ca6-a74e-ace4fd914626",
   "metadata": {
    "id": "32d6bf1c-a802-4ca6-a74e-ace4fd914626"
   },
   "source": [
    "### Task 4 (1 point)\n",
    "\n",
    "a) Show how many articles we have for each topical area (class label) in the dataset using a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ef9d8-52c2-4508-ad94-91525476d9a4",
   "metadata": {
    "id": "c93ef9d8-52c2-4508-ad94-91525476d9a4"
   },
   "outputs": [],
   "source": [
    "# Here comes your code\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd667360-ae62-4f70-8236-9e9d11ab29d7",
   "metadata": {
    "id": "cd667360-ae62-4f70-8236-9e9d11ab29d7"
   },
   "source": [
    "### Task 5 (2 point)\n",
    "\n",
    "Preprocessing: Define two following functions and apply them to the dataset:\n",
    "1. Remove punctuation\n",
    "2. Remove any numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d69ff-c80a-468e-a6be-af5538a8c2b9",
   "metadata": {
    "id": "8f3d69ff-c80a-468e-a6be-af5538a8c2b9"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(corpus):\n",
    "    # Here comes your code\n",
    "    return(cleaned_corpus)\n",
    "\n",
    "def remove_numbers(corpus):\n",
    "    # Here comes your code\n",
    "    return(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fc751-360f-45c8-8528-9dc610cad362",
   "metadata": {
    "id": "181fc751-360f-45c8-8528-9dc610cad362"
   },
   "source": [
    "### Task 6 (4 points)\n",
    "\n",
    "a) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\n",
    "\n",
    "b) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\n",
    "\n",
    "c) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n",
    "\n",
    "**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619b0ed-2213-41de-bf9f-95c97344f038",
   "metadata": {
    "id": "3619b0ed-2213-41de-bf9f-95c97344f038"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a53269-9dc6-43a8-a071-dc596beee0cf",
   "metadata": {
    "id": "88a53269-9dc6-43a8-a071-dc596beee0cf"
   },
   "source": [
    "### Task 7 (6 points)\n",
    "\n",
    "a) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\n",
    "\n",
    "b) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80276a8-e395-4015-a5d8-00fa7b8e051b",
   "metadata": {
    "id": "d80276a8-e395-4015-a5d8-00fa7b8e051b"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
